---
title: "Data Request & Ingestion"
slug: "05-data-request-and-ingestion"
estimated_duration_minutes: 6
order: 5
---

# Data Request & Ingestion

Once the Issues Tree and initial hypotheses are established, the practitioner must formally request and ingest the data needed to test them. This step bridges the gap between analytical planning and actual analysis.

## The Formal Data Request

A data request should be specific, structured, and traceable back to the Issues Tree. Each request should include:

- **Data Description** — What specific data is needed (e.g., "dispatch records for the past 12 months including technician ID, job type, travel time, and resolution outcome")
- **Source System** — Where does this data reside? Identify the system of record.
- **Format Requirements** — Preferred file format, schema expectations, and any transformation needs
- **Time Period** — The date range required for the analysis
- **Granularity** — The level of detail needed (e.g., daily vs. monthly, individual vs. aggregated)
- **Issues Tree Mapping** — Which branch or hypothesis does this data support?

<Callout type="concept" title="Metro Cable Example">
For the Metro Cable engagement, the initial data request included 14 months of dispatch records (350,000+ rows), customer satisfaction survey results, technician certification databases, and geographic service area maps. This broad initial request was intentional — casting a wide net in the first iteration enables AI to surface unexpected patterns.
</Callout>

<DataRequestList />

## Data Ingestion Best Practices

Once data is received, ingestion involves loading it into the AI analysis environment and performing initial quality checks:

1. **Profile the data** — Use AI to generate summary statistics, identify missing values, and flag anomalies
2. **Validate schema** — Confirm that field names, data types, and formats match expectations
3. **Establish a baseline** — Create initial descriptive statistics that serve as reference points for analysis
4. **Document limitations** — Record known data quality issues, gaps, or caveats

<DataRequestRisks />

<Callout type="tip" title="More Data Is Better">
In AOMT, the bias should always be toward requesting more data rather than less. AI tools can process large volumes efficiently, and datasets that seem peripheral often contain unexpected insights. The cost of ingesting extra data is minimal compared to the cost of missing a critical pattern because the data was never requested.
</Callout>

<Callout type="warning" title="Data Governance">
Always follow your organization's data governance policies when requesting and handling data. Ensure appropriate access permissions, anonymization requirements, and retention policies are observed throughout the engagement.
</Callout>

<RefinedDataRequest />

<DecisionPoint
  scenario="You're starting a new AOMT engagement. The client offers two data options: (A) a clean, curated dataset covering 3 months of operations, or (B) a raw, uncleaned dataset covering 18 months with known quality issues. Which do you choose?"
  options={[
    { id: "curated", label: "Option A: Clean 3-month dataset", outcome: "While easier to work with, 3 months may not capture seasonal patterns, trends, or rare events. You risk missing critical insights that only emerge over longer time periods." },
    { id: "raw", label: "Option B: Raw 18-month dataset", outcome: "AI tools can handle data cleaning efficiently. The longer time period reveals seasonal patterns, trends, and edge cases. The initial effort to clean the data is far outweighed by the analytical depth it enables.", recommended: true },
    { id: "both", label: "Request both datasets", outcome: "This is a reasonable approach — use the clean dataset for quick initial analysis while the raw data is being profiled and cleaned. However, ensure the 3-month curated data doesn't bias your initial hypotheses." }
  ]}
/>
